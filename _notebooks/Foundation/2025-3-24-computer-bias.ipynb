{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "layout: post\n",
    "title: Computer Bias\n",
    "description: Completing Hacks for Computer Bias\n",
    "permalink: /computer/bias\n",
    "menu: nav/tools_setup.html\n",
    "toc: true\n",
    "comments: true \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popcorn Hack #1\n",
    "\n",
    "### Provide an example of a movie, TV show, video game, or software that demonstrates bias and specify who is affected by it. Explain a potential cause of this bias.\n",
    "\n",
    "- A good example of bias is in the video game Grand Theft Auto V (GTA V). In the game, people of color and poor neighborhoods are often shown in negative ways, like being involved in gangs or crime. This can give players the wrong idea about these groups in real life. One reason for this bias might be that the people who made the game didn’t include enough different voices or do enough research. Instead, they may have just used stereotypes, which can lead to unfair and harmful messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popcorn Hack #2\n",
    "\n",
    "### Think about a time when you felt a technology didn't work well for you. What was the issue, and how did it make you feel? Write a short paragraph describing the experience and suggest one way the technology could be improved to be more inclusive.\n",
    "\n",
    "- One time, I was using a voice-to-text app to take notes for school, but it kept getting my words wrong because of my accent. It was really frustrating because I had to go back and fix everything, which took more time than just writing it myself. It made me feel like the app wasn’t made for people like me. To make it more inclusive, the app could be trained to understand different accents better by using a more diverse group of voices during testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popcorn Hack #3\n",
    "\n",
    "Imagine you're designing a fitness tracking app. How could bias sneak into your app’s recommendations or performance evaluations? Think about users with different physical abilities, ages, or health conditions. What features could you add to ensure the app is fair and inclusive for all users?\n",
    "\n",
    "- Bias could sneak into a fitness tracking app if it gives the same workout goals to everyone without thinking about people’s different bodies, ages, or health needs. For example, an older person or someone with a disability might not be able to run or lift weights like a younger or fully able person. If the app only praises high-intensity workouts, it could make some users feel left out or like they’re not doing enough. To make the app fair, I would add a setup that asks about the user’s age, ability level, and health conditions. Then, the app could give custom goals and celebrate all kinds of progress, not just the most intense workouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Hack\n",
    "\n",
    "Choose a digital tool, app, or website you use regularly. It could be a social media platform, a shopping site, or a streaming service.\n",
    "Identify Potential Bias: Are there any patterns in the recommendations or interactions that might suggest bias? Does the platform cater well to different user groups (e.g., age, gender, language, accessibility)?\n",
    "Analyze the Cause: What might be causing this bias? Consider data collection, algorithm design, or lack of diverse testing.\n",
    "Propose a Solution: Suggest one way the developers could reduce bias and make the platform more inclusive.\n",
    "\n",
    "- A digital tool I use a lot is YouTube. I’ve noticed that it often recommends videos based on what I’ve already watched, which can create a bias. For example, if someone mostly watches videos from one type of creator or topic, YouTube will keep recommending more of the same, instead of showing different opinions or types of content. This might make it hard for people to see new things or learn from different groups. One cause of this bias is how the algorithm is made to keep people watching, so it shows what it thinks we like instead of new ideas. To fix this, YouTube could add a feature that lets users choose to see a wider variety of content, or suggest videos from different cultures, age groups, or viewpoints to make the platform more fair and open to everyone."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
